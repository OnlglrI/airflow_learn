"""
ÐšÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€: ETL pipeline Ñ PostgreSQL Ð¸ MinIO
Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ»: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PostgreSQL,
Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² MinIO
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.models import Variable
from datetime import datetime, timedelta
import json
import pandas as pd
from io import StringIO

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


# Shared config
POSTGRES_CONN_ID = Variable.get("POSTGRES_CONN_ID", default_var="postgres_data")
MINIO_CONN_ID = Variable.get("MINIO_CONN_ID", default_var="minio")
SALES_BUCKET = Variable.get("SALES_BUCKET", default_var="sales-reports")
SALES_RAW_PREFIX = Variable.get("SALES_RAW_PREFIX", default_var="raw_data")
SALES_REPORTS_PREFIX = Variable.get("SALES_REPORTS_PREFIX", default_var="reports")


def create_sample_table(**context):
    """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð² PostgreSQL"""
    postgres_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
    sql = """
    DROP TABLE IF EXISTS sales_data;
    
    CREATE TABLE sales_data (
        id SERIAL PRIMARY KEY,
        product_name VARCHAR(100),
        category VARCHAR(50),
        price DECIMAL(10, 2),
        quantity INTEGER,
        sale_date DATE,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    
    INSERT INTO sales_data (product_name, category, price, quantity, sale_date)
    VALUES 
        ('Laptop', 'Electronics', 999.99, 5, CURRENT_DATE - INTERVAL '1 day'),
        ('Mouse', 'Electronics', 29.99, 15, CURRENT_DATE - INTERVAL '1 day'),
        ('Keyboard', 'Electronics', 79.99, 10, CURRENT_DATE - INTERVAL '1 day'),
        ('Chair', 'Furniture', 199.99, 3, CURRENT_DATE - INTERVAL '1 day'),
        ('Desk', 'Furniture', 299.99, 2, CURRENT_DATE - INTERVAL '1 day'),
        ('Monitor', 'Electronics', 249.99, 7, CURRENT_DATE),
        ('Headphones', 'Electronics', 149.99, 12, CURRENT_DATE),
        ('Lamp', 'Furniture', 49.99, 8, CURRENT_DATE);
    """

    postgres_hook.run(sql)
    print("âœ… Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° sales_data ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð¸ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°")


def extract_from_postgres(**context):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PostgreSQL"""
    postgres_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

    sql = """
    SELECT 
        product_name,
        category,
        price,
        quantity,
        price * quantity as total_amount,
        sale_date
    FROM sales_data
    WHERE sale_date >= CURRENT_DATE - INTERVAL '7 days'
    ORDER BY sale_date DESC, total_amount DESC;
    """

    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ°Ðº pandas DataFrame
    df = postgres_hook.get_pandas_df(sql)

    print(f"ðŸ“Š Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    print("\nÐŸÐµÑ€Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸:")
    print(df.head())

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² XCom ÐºÐ°Ðº JSON
    return df.to_json(orient='records', date_format='iso')


def transform_data(**context):
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    ti = context['ti']
    data_json = ti.xcom_pull(task_ids='extract_from_postgres')

    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² DataFrame
    df = pd.read_json(StringIO(data_json))

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÑƒ
    summary = {
        'total_records': len(df),
        'total_revenue': float(df['total_amount'].sum()),
        'average_price': float(df['price'].mean()),
        'categories': df['category'].value_counts().to_dict(),
        'top_products': df.nlargest(3, 'total_amount')[['product_name', 'total_amount']].to_dict('records'),
        'report_date': datetime.now().isoformat()
    }

    print("ðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
    print(f"  Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {summary['total_records']}")
    print(f"  ÐžÐ±Ñ‰Ð°Ñ Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ°: ${summary['total_revenue']:.2f}")
    print(f"  Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ñ†ÐµÐ½Ð°: ${summary['average_price']:.2f}")

    return {
        'data': data_json,
        'summary': summary
    }


def load_to_minio(**context):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² MinIO"""
    ti = context['ti']
    result = ti.xcom_pull(task_ids='transform_data')

    s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)
    bucket_name = SALES_BUCKET

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ bucket ÐµÑÐ»Ð¸ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
    if not s3_hook.check_for_bucket(bucket_name):
        s3_hook.create_bucket(bucket_name=bucket_name)
        print(f"âœ… Bucket '{bucket_name}' ÑÐ¾Ð·Ð´Ð°Ð½")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑ‹Ñ€Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    execution_date = context['ds']
    data_key = f'{SALES_RAW_PREFIX}/{execution_date}/sales_data.json'
    s3_hook.load_string(
        string_data=result['data'],
        key=data_key,
        bucket_name=bucket_name,
        replace=True
    )
    print(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: {data_key}")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚
    report_key = f'{SALES_REPORTS_PREFIX}/{execution_date}/summary.json'
    s3_hook.load_string(
        string_data=json.dumps(result['summary'], indent=2),
        key=report_key,
        bucket_name=bucket_name,
        replace=True
    )
    print(f"âœ… ÐžÑ‚Ñ‡ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½: {report_key}")

    return {
        'bucket': bucket_name,
        'data_key': data_key,
        'report_key': report_key
    }


def send_notification(**context):
    """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸"""
    ti = context['ti']
    result = ti.xcom_pull(task_ids='load_to_minio')
    summary = ti.xcom_pull(task_ids='transform_data')['summary']

    message = f"""
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ðŸ“Š ETL Pipeline Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    ðŸ“… Ð”Ð°Ñ‚Ð°: {context['ds']}
    
    ðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:
      â€¢ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {summary['total_records']}
      â€¢ ÐžÐ±Ñ‰Ð°Ñ Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ°: ${summary['total_revenue']:.2f}
      â€¢ Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ñ†ÐµÐ½Ð°: ${summary['average_price']:.2f}
    
    ðŸ“¦ Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² MinIO:
      â€¢ Bucket: {result['bucket']}
      â€¢ Ð”Ð°Ð½Ð½Ñ‹Ðµ: {result['data_key']}
      â€¢ ÐžÑ‚Ñ‡ÐµÑ‚: {result['report_key']}
    
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """

    print(message)
    return "Success"


with DAG(
    'etl_postgres_to_minio',
    default_args=default_args,
    description='ETL pipeline: PostgreSQL â†’ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° â†’ MinIO',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'postgres', 'minio', 'production'],
) as dag:

    create_table_task = PythonOperator(
        task_id='create_sample_table',
        python_callable=create_sample_table,
    )

    extract_task = PythonOperator(
        task_id='extract_from_postgres',
        python_callable=extract_from_postgres,
    )

    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
    )

    load_task = PythonOperator(
        task_id='load_to_minio',
        python_callable=load_to_minio,
    )

    notify_task = PythonOperator(
        task_id='send_notification',
        python_callable=send_notification,
    )

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
    create_table_task >> extract_task >> transform_task >> load_task >> notify_task

