# Scheduler (Планировщик)

Планировщик Airflow отслеживает все задачи и DAG’и, а затем запускает экземпляры задач, как только их зависимости будут выполнены. Внутри планировщик запускает подпроцесс, который отслеживает и поддерживает синхронизацию со всеми DAG’ами в указанной директории DAG. По умолчанию один раз в минуту планировщик собирает результаты парсинга DAG’ов и проверяет, можно ли запустить какие-либо активные задачи.

Планировщик Airflow спроектирован для работы как постоянно запущенный сервис в production-окружении Airflow. Чтобы запустить его, достаточно выполнить команду `airflow scheduler`. Он использует конфигурацию, указанную в файле `airflow.cfg`.

Планировщик использует настроенный Executor для запуска задач, которые готовы к выполнению.

Чтобы запустить планировщик, выполните команду:

```bash
airflow scheduler
```

После успешного запуска планировщика ваши DAG’и начнут выполняться.

---

## Примечание

Первый запуск DAG (DAG Run) создаётся на основе минимального значения `start_date` среди задач в DAG’е. Последующие DAG Run’ы создаются в соответствии с расписанием (timetable) DAG’а.

Для DAG’ов с расписанием в виде cron или `timedelta` планировщик не запустит задачи, пока период, который они покрывают, не завершится. Например, задача с расписанием `@daily` будет запущена после окончания дня. Такой подход гарантирует, что все данные, необходимые за этот период, будут полностью доступны до выполнения DAG’а. В интерфейсе UI это выглядит так, будто Airflow запускает задачи с задержкой в один день.

---

## Примечание

Если DAG запускается с дневным расписанием, запуск с интервалом данных, начинающимся `2019-11-21`, будет инициирован после `2019-11-21T23:59`.

---

Повторим ещё раз: **планировщик запускает ваш DAG на одно расписание ПОСЛЕ даты начала, в КОНЦЕ интервала**.

Для получения подробностей о планировании DAG’ов см. раздел *DAG Runs*.

---

## Обработка DAG-файлов

Вы можете настроить планировщик Airflow так, чтобы он отвечал за запуск процесса, который преобразует Python-файлы в папке DAG’ов в объекты DAG, содержащие задачи для планирования.

Подробности см. в разделе *DAG File Processing*.

---

## Запуск нескольких планировщиков

Airflow поддерживает одновременную работу нескольких планировщиков — как для повышения производительности, так и для отказоустойчивости.

Это, однако, накладывает определённые требования на базу данных.

---

# Тонкая настройка производительности планировщика

## Что влияет на производительность планировщика

Планировщик отвечает за две операции:

* непрерывный парсинг DAG-файлов и синхронизацию с DAG’ами в базе данных;
* непрерывное планирование задач для выполнения.

---



## Что можно сделать для улучшения производительности планировщика

* оптимизировать верхнеуровневый код DAG’ов;
* улучшить использование доступных ресурсов;
* увеличить аппаратные ресурсы;
* экспериментировать с параметрами планировщика;
* при необходимости менять поведение планировщика (например, порядок парсинга).

---

## Параметры конфигурации планировщика

Следующие параметры позволяют управлять поведением планировщика:

* **max_dagruns_to_create_per_loop** — количество DAG’ов, блокируемых при создании DAG Run’ов.
* **max_dagruns_per_loop_to_schedule** — количество DAG Run’ов, рассматриваемых за один цикл.
* **use_row_level_locking** — использование `SELECT … FOR UPDATE`.
* **pool_metrics_interval** — интервал отправки метрик пулов в StatsD.
* **orphaned_tasks_check_interval** — интервал проверки «осиротевших» задач.
* **dag_dir_list_interval** — интервал сканирования директории DAG’ов.
* **file_parsing_sort_mode** — порядок сортировки DAG-файлов.
* **max_tis_per_query** — размер пакета задач в SQL-запросе.
* **min_file_process_interval** — интервал повторного парсинга DAG-файлов.
* **parsing_processes** — количество процессов парсинга.
* **scheduler_idle_sleep_time** — время сна планировщика между циклами.
* **schedule_after_task_execution** — мини-планирование задач внутри DAG’а после выполнения задачи.

---

